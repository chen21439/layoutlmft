from dataclasses import dataclass, field
from typing import Optional


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    task_name: Optional[str] = field(default="ner", metadata={"help": "The name of the task (ner, pos...)."})
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training data file (a csv or JSON file)."}
    )
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate on (a csv or JSON file)."},
    )
    test_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input test data file to predict on (a csv or JSON file)."},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    pad_to_max_length: bool = field(
        default=True,
        metadata={
            "help": "Whether to pad all samples to model maximum sentence length. "
            "If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
            "efficient on GPU but very bad for TPU."
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
            "value if set."
        },
    )
    max_val_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of validation examples to this "
            "value if set."
        },
    )
    max_test_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of test examples to this "
            "value if set."
        },
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            "help": "Whether to put the label for one word on all tokens of generated by that word or just on the "
            "one (in which case the other tokens will have a padding index)."
        },
    )
    return_entity_level_metrics: bool = field(
        default=False,
        metadata={"help": "Whether to return all the entity levels during evaluation or just the overall ones."},
    )

    # Balanced loss settings for long-tailed classification
    loss_type: str = field(
        default="class_balanced",  # Default to Class-Balanced Loss (Effective Number) for long-tailed classification
        metadata={
            "help": "Loss function type: 'ce' (CrossEntropy), 'class_balanced', 'logit_adjusted', "
                    "'focal', 'balanced_focal'. Recommended: 'class_balanced' or 'logit_adjusted'"
        },
    )
    loss_beta: float = field(
        default=0.9999,
        metadata={"help": "Beta for Class-Balanced Loss (effective number). Higher = more balanced."},
    )
    loss_gamma: float = field(
        default=2.0,
        metadata={"help": "Gamma for Focal Loss. Higher = more focus on hard examples."},
    )
    loss_tau: float = field(
        default=1.0,
        metadata={"help": "Tau for Logit Adjustment. 1.0 = Balanced Softmax."},
    )

    # Class-balanced batch sampling (Step 3)
    use_class_balanced_sampler: bool = field(
        default=True,  # Default enabled for long-tailed classification
        metadata={"help": "Use class-balanced batch sampler to ensure coverage of rare classes."},
    )
    rare_class_ratio: float = field(
        default=0.3,
        metadata={"help": "Fraction of each batch that should contain rare class samples."},
    )


@dataclass
class XFUNDataTrainingArguments(DataTrainingArguments):
    lang: Optional[str] = field(default="en")
    additional_langs: Optional[str] = field(default=None)
