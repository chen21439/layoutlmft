# è®ºæ–‡å‚æ•°å¯¹é½è¯´æ˜

HRDocè®ºæ–‡è®­ç»ƒå‚æ•°çš„å®Œæ•´å¯¹é½æƒ…å†µã€‚

---

## ğŸ“Š è®ºæ–‡ vs å½“å‰é…ç½®å¯¹æ¯”

### HRDoc-Simple

| å‚æ•° | è®ºæ–‡å€¼ | cloudé…ç½® | çŠ¶æ€ |
|------|--------|----------|------|
| **Training Steps** | 30,000 | **30,000** | âœ… å®Œå…¨ä¸€è‡´ |
| **Batch Size** | 3 (page-level) | **3** | âœ… å®Œå…¨ä¸€è‡´ |
| **Gradient Accumulation** | æ¨æµ‹=1 | **1** | âœ… å¯¹é½ |
| **Training Time** | ~4.5å°æ—¶ | ~4.5å°æ—¶ | âœ… é¢„æœŸä¸€è‡´ |
| **Hardware** | V100-24G | - | - |
| **Learning Rate** | æœªå…¬å¼€ | **5e-5** | âš ï¸ LayoutLMv2æ ‡å‡†å€¼ |
| **Warmup Ratio** | æœªå…¬å¼€ | **0.1** | âš ï¸ HFå¸¸ç”¨å€¼ |
| **Weight Decay** | æœªå…¬å¼€ | **0.01** | âš ï¸ BERT/LayoutLMæ ‡å‡† |
| **LR Scheduler** | æœªå…¬å¼€ | **linear** | âš ï¸ HFé»˜è®¤ |
| **Optimizer** | æœªå…¬å¼€ | **AdamW** | âš ï¸ HFé»˜è®¤ |
| **FP16** | æ¨æµ‹ä½¿ç”¨ | **True** | âœ… å¯¹é½ç¡¬ä»¶ç¯å¢ƒ |

### HRDoc-Hard

| å‚æ•° | è®ºæ–‡å€¼ | cloud_hardé…ç½® | çŠ¶æ€ |
|------|--------|---------------|------|
| **Training Steps** | 40,000 | **40,000** | âœ… å®Œå…¨ä¸€è‡´ |
| **Batch Size** | 3 (page-level) | **3** | âœ… å®Œå…¨ä¸€è‡´ |
| **Training Time** | ~6å°æ—¶ | ~6å°æ—¶ | âœ… é¢„æœŸä¸€è‡´ |
| å…¶ä»–å‚æ•°åŒSimple | - | åŒä¸Š | - |

---

## ğŸ“„ è®ºæ–‡åŸæ–‡å¼•ç”¨

> "We trained LayoutLMv2 on the HRDoc-Simple dataset with a **batch size of 3 (page-level) for 30,000 steps**, the training stage costs about **4.5 hours** on single NVIDIA V100-24G GPU."
>
> â€” [HRDoc GitHub README](https://github.com/jfma-USTC/HRDoc)

> "We trained LayoutLMv2 on the HRDoc-Hard dataset with a **batch size of 3 (page-level) for 40,000 steps**, the training stage costs about **6 hours** on single NVIDIA V100-24G GPU."
>
> â€” [HRDoc GitHub README](https://github.com/jfma-USTC/HRDoc)

---

## âš™ï¸ æœªå…¬å¼€å‚æ•°çš„å¤„ç†ç­–ç•¥

è®ºæ–‡å’Œå®˜æ–¹ä»“åº“**æœªå…¬å¼€**ä»¥ä¸‹å…³é”®è¶…å‚æ•°ï¼š

- Learning Rate
- Warmupç­–ç•¥
- Weight Decay
- LR Schedulerç±»å‹
- ä¼˜åŒ–å™¨é€‰æ‹©

### è§£å†³æ–¹æ¡ˆ

åŸºäºä»¥ä¸‹ä¾æ®å¡«è¡¥ï¼š

1. **LayoutLMv2å®˜æ–¹ç¤ºä¾‹**
2. **layoutlmfté¡¹ç›®é»˜è®¤é…ç½®**
3. **BERT/Transformerå¾®è°ƒæœ€ä½³å®è·µ**
4. **HuggingFace Traineré»˜è®¤å€¼**

å…·ä½“é€‰æ‹©ï¼š

```python
learning_rate = 5e-5        # LayoutLMç³»åˆ—å¸¸ç”¨finetune lr
warmup_ratio = 0.1          # å‰10% stepsåšwarmupï¼ˆ3000æ­¥ï¼‰
weight_decay = 0.01         # BERTæ ‡å‡†å€¼
lr_scheduler_type = "linear" # HF Traineré»˜è®¤
optimizer = "AdamW"         # HF Traineré»˜è®¤
```

### å‚è€ƒæ¥æº

- [Hugging Face TrainingArgumentsæ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)
- [LayoutLMv2è®ºæ–‡](https://arxiv.org/abs/2012.14740)
- [layoutlmftç¤ºä¾‹](https://github.com/microsoft/unilm/tree/master/layoutlmft)

---

## ğŸ”¬ éªŒè¯æ–¹æ³•

è®­ç»ƒå®Œæˆåï¼Œæ£€æŸ¥ä»¥ä¸‹æ–‡ä»¶ç¡®è®¤é…ç½®æ­£ç¡®ï¼š

### 1. `trainer_state.json`

```bash
cat ./output/hrdoc_simple_full/trainer_state.json | grep -E "max_steps|global_step"
```

åº”è¾“å‡ºï¼š
```json
"max_steps": 30000,
"global_step": 30000  // æˆ–æ¥è¿‘30000
```

### 2. `training_args.bin`

```python
import torch
args = torch.load('./output/hrdoc_simple_full/training_args.bin')
print(f"batch_size: {args.per_device_train_batch_size}")  # åº”ä¸º 3
print(f"learning_rate: {args.learning_rate}")            # åº”ä¸º 5e-5
print(f"warmup_ratio: {args.warmup_ratio}")              # åº”ä¸º 0.1
print(f"weight_decay: {args.weight_decay}")              # åº”ä¸º 0.01
```

---

## ğŸ“ˆ é¢„æœŸè®­ç»ƒæ›²çº¿

åŸºäºè®ºæ–‡å’Œå®è·µç»éªŒï¼š

### HRDoc-Simple (30kæ­¥)

| Metric | é¢„æœŸå€¼ | å¤‡æ³¨ |
|--------|-------|------|
| Final F1 | ~98% | è®ºæ–‡è¡¨æ ¼ä¸­Simpleæ•°æ®é›†F1 |
| Training Loss | ä¸‹é™è‡³<0.1 | æ­£å¸¸æ”¶æ•› |
| Eval F1 (å³°å€¼) | 98-99% | å¯èƒ½åœ¨åæœŸç•¥æœ‰æ³¢åŠ¨ |

### HRDoc-Hard (40kæ­¥)

| Metric | é¢„æœŸå€¼ | å¤‡æ³¨ |
|--------|-------|------|
| Final F1 | ~95% | è®ºæ–‡è¡¨æ ¼ä¸­Hardæ•°æ®é›†F1 |
| Training Loss | ä¸‹é™è‡³<0.2 | Hardæ•°æ®é›†éš¾åº¦æ›´é«˜ |
| Eval F1 (å³°å€¼) | 95-97% | æ³¢åŠ¨å¯èƒ½æ›´æ˜æ˜¾ |

---

## ğŸš€ å¯åŠ¨å‘½ä»¤

### ä½¿ç”¨é…ç½®æ–‡ä»¶

```bash
# HRDoc-Simple (30kæ­¥)
python train.py --config configs/cloud_config.json

# HRDoc-Hard (40kæ­¥)
python train.py --config configs/cloud_hard_config.json
```

### ä½¿ç”¨ä¾¿æ·è„šæœ¬

```bash
# HRDoc-Simple
./train_hrdoc_official.sh simple

# HRDoc-Hard
./train_hrdoc_official.sh hard
```

### ç›´æ¥å‘½ä»¤è¡Œ

```bash
# HRDoc-Simple
python examples/run_hrdoc.py \
  --model_name_or_path microsoft/layoutlmv2-base-uncased \
  --output_dir ./output/hrdoc_simple_full \
  --do_train --do_eval \
  --max_steps 30000 \
  --per_device_train_batch_size 3 \
  --per_device_eval_batch_size 8 \
  --learning_rate 5e-5 \
  --warmup_ratio 0.1 \
  --weight_decay 0.01 \
  --logging_steps 100 \
  --eval_steps 1000 \
  --save_steps 1000 \
  --evaluation_strategy steps \
  --save_total_limit 3 \
  --fp16 \
  --overwrite_output_dir

# HRDoc-Hard: åªéœ€æ”¹ max_steps å’Œ output_dir
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### æ˜¾å­˜è¦æ±‚

- **æ¨èé…ç½®**: 20GB+ GPUæ˜¾å­˜ (V100/A100)
- **æœ€ä½é…ç½®**: 16GB (å¯èƒ½éœ€è¦è°ƒæ•´batch_size)

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼š

```bash
# æ–¹æ¡ˆ1: å‡å°batch_size + å¢åŠ æ¢¯åº¦ç´¯ç§¯
--per_device_train_batch_size 2 \
--gradient_accumulation_steps 2    # æœ‰æ•ˆbatch=2Ã—2=4 (æ¥è¿‘è®ºæ–‡çš„3)

# æ–¹æ¡ˆ2: å…³é—­FP16ï¼ˆä¸æ¨èï¼Œä¼šæ›´æ…¢ï¼‰
# ç§»é™¤ --fp16 å‚æ•°
```

### è®­ç»ƒä¸­æ–­æ¢å¤

å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä»¥ä»checkpointæ¢å¤ï¼š

```bash
# ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹æœ€æ–°checkpoint
python examples/run_hrdoc.py \
  --output_dir ./output/hrdoc_simple_full \
  ...ï¼ˆå…¶ä»–å‚æ•°åŒä¸Šï¼‰
  # ç§»é™¤ --overwrite_output_dir
```

---

## ğŸ“Š ä¸50æ­¥æµ‹è¯•é…ç½®çš„å¯¹æ¯”

| å‚æ•° | æµ‹è¯•é…ç½® | è®ºæ–‡é…ç½® | å·®è· |
|------|---------|---------|------|
| max_steps | 50 | 30,000 | **600å€** |
| è®­ç»ƒæ—¶é•¿ | ~8åˆ†é’Ÿ | ~4.5å°æ—¶ | **34å€** |
| batch_size | æœªçŸ¥ | 3 | - |
| F1æ€§èƒ½ | ä½ï¼ˆè®­ç»ƒä¸å……åˆ†ï¼‰ | ~98% | æ˜¾è‘—å·®è· |

**ç»“è®º**: ä¹‹å‰çš„50æ­¥é…ç½®ä»…ç”¨äºä»£ç è°ƒè¯•ï¼Œ**ä¸èƒ½**ç”¨äºæ­£å¼å®éªŒæˆ–è®ºæ–‡å¤ç°ã€‚

---

## ğŸ”— å‚è€ƒèµ„æ–™

- **HRDocè®ºæ–‡**: [arXiv:2303.13839](https://arxiv.org/abs/2303.13839)
- **HRDoc GitHub**: [jfma-USTC/HRDoc](https://github.com/jfma-USTC/HRDoc)
- **LayoutLMv2è®ºæ–‡**: [arXiv:2012.14740](https://arxiv.org/abs/2012.14740)
- **HuggingFace Traineræ–‡æ¡£**: [transformers.Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
