# Local Development Configuration
# For local machine development and debugging

# Environment identifier
env: dev
description: "Local Development - Windows WSL"

# Dataset configuration
dataset:
  # Which dataset to use: hrds, hrdh, or tender
  name: "hrds"
  # Dataset base directory (datasets are subdirectories: HRDS, HRDH, tender_document, etc.)
  base_dir: "/mnt/e/models/data/Section"
  # Covmatch split directory name
  covmatch: "doc_covmatch_dev10_seed42"

# Data paths
paths:
  # HRDoc dataset root directory (legacy, for backward compatibility)
  hrdoc_data_dir: "/mnt/e/models/data/Section/HRDS"

  # Stage 1: Fine-tuned LayoutLMv2 model checkpoint
  stage1_model_path: "/mnt/e/models/train_data/layoutlmft/exp_20251212_172343/stage1_hrdh/checkpoint-5500"

  # Feature files directory (Stage 2 input/output)
  features_dir: "/mnt/e/models/train_data/layoutlmft/line_features_doc"

  # Model output root directory
  output_dir: "/mnt/e/models/train_data/layoutlmft"

  # HuggingFace model cache
  hf_cache_dir: "/mnt/e/models/HuggingFace/hub"

# Model configuration
model:
  # Pretrained model (from HuggingFace)
  name_or_path: "microsoft/layoutxlm-base"
  # Local downloaded model path (optional, takes priority)
  local_path: "/mnt/e/models/HuggingFace/hub/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a"

# Metrics configuration
metrics:
  # Local seqeval metric script path (for offline mode)
  seqeval_path: "/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.py"

# Stage 1: LayoutXLM fine-tuning parameters
stage1_training:
  # Training loop
  max_steps: 500
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2

  # Optimizer settings
  learning_rate: 5.0e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  lr_scheduler_type: "linear"
  warmup_ratio: 0.1

  # Evaluation & checkpointing
  evaluation_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "macro_f1"
  greater_is_better: true

  # Logging
  logging_steps: 20
  logging_first_step: true

  # Stability & other
  fp16: true
  dataloader_num_workers: 2
  seed: 42

# Stage 2: Feature extraction parameters
feature_extraction:
  batch_size: 50
  docs_per_chunk: 100
  num_samples: -1  # -1 means all, positive number limits samples

# Stage 3: ParentFinder training parameters
parent_finder:
  mode: "full"  # simple or full
  level: "document"  # page or document
  max_lines_limit: 512
  batch_size: 1
  num_epochs: 20
  learning_rate: 1.0e-4
  max_chunks: -1  # -1 means all

# Stage 4: Relation classifier training parameters
relation_classifier:
  max_steps: 300
  batch_size: 32
  learning_rate: 5.0e-4
  neg_ratio: 1.5
  max_chunks: -1

# Evaluation configuration
# Inference results are written to dataset directory (test_infer_stage1 / test_infer_e2e)
# These folders are auto-created and cleared before each inference run
evaluation:
  # Stage 1 inference output: {data_dir}/test_infer_stage1
  stage1_infer_folder: "test_infer_stage1"
  # End-to-end inference output: {data_dir}/test_infer_e2e
  e2e_infer_folder: "test_infer_e2e"

# Quick test configuration (overrides above parameters)
quick_test:
  enabled: true
  num_samples: 10
  docs_per_chunk: 5
  stage1_max_steps: 10
  parent_finder_epochs: 1
  relation_max_steps: 10

# 推理配置 (FastAPI 服务使用)
inference:
  checkpoint_path: "/mnt/e/models/layoutlmft/exp_20251227_161829/joint_tender/checkpoint-375"
  data_dir_base: "/mnt/e/programFile/AIProgram/tender_ontology/static/upload"
