# Cloud Server Test Configuration
# For testing and training on cloud server

# Environment identifier
env: test
description: "Cloud Server Test - Ubuntu"

# GPU configuration
gpu:
  cuda_visible_devices: "2"  # Specify which GPU to use (e.g., "0", "0,1", "2")

# Data paths
paths:
  # HRDoc dataset root directory
  hrdoc_data_dir: "/data/LLM_group/layoutlmft/data/HRDS"

  # Stage 1: Fine-tuned LayoutLMv2 model checkpoint
  stage1_model_path: "/data/LLM_group/layoutlmft/artifact/stage1"

  # Feature files directory (Stage 2 input/output)
  features_dir: "/data/LLM_group/layoutlmft/artifact/line_features"

  # Model output root directory
  output_dir: "/data/LLM_group/layoutlmft/artifact"

  # HuggingFace model cache
  hf_cache_dir: "/data/LLM_group/models/huggingface"

# Model configuration
model:
  # Pretrained model (from HuggingFace)
  name_or_path: "microsoft/layoutxlm-base"
  # Local downloaded model path (for offline mode)
  local_path: "/data/LLM_group/HuggingFace/Hub/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a"

# Metrics configuration
metrics:
  # Local seqeval metric script path (for offline mode)
  seqeval_path: "/data/LLM_group/HuggingFace/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.py"

# Stage 1: LayoutXLM fine-tuning parameters
stage1_training:
  max_steps: 30000  # Full training
  per_device_train_batch_size: 3  # Aligned with paper (batch_size=3)
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 2.0e-5  # Reduced from 5e-5 for stability
  warmup_ratio: 0.1
  weight_decay: 0.01
  logging_steps: 100
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 3
  fp16: false  # Disabled for stability (FP16 may cause NaN on new environment)
  dataloader_num_workers: 4
  seed: 42

# Stage 2: Feature extraction parameters
feature_extraction:
  batch_size: 50
  docs_per_chunk: 100
  num_samples: -1  # -1 means all

# Stage 3: ParentFinder training parameters
parent_finder:
  mode: "full"
  level: "document"
  max_lines_limit: 512
  batch_size: 1
  num_epochs: 20
  learning_rate: 1.0e-4
  max_chunks: -1

# Stage 4: Relation classifier training parameters
relation_classifier:
  max_steps: 300
  batch_size: 32
  learning_rate: 5.0e-4
  neg_ratio: 1.5
  max_chunks: -1

# Quick test configuration (overrides above parameters)
quick_test:
  enabled: false  # Set to true for quick testing, false for full training
  num_samples: 10
  docs_per_chunk: 5
  stage1_max_steps: 50
  parent_finder_epochs: 1
  relation_max_steps: 50
