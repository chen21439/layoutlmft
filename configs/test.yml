# Cloud Server Test Configuration
# For testing and training on cloud server

# Environment identifier
env: test
description: "Cloud Server Test - Ubuntu"

# GPU configuration
gpu:
  cuda_visible_devices: "2"  # Specify which GPU to use (e.g., "0", "0,1", "2")

# Experiment configuration
experiment:
  # auto_create: If true, automatically create new experiment when none exists
  auto_create: true
  # default_name: Default name for auto-created experiments
  default_name: "LayoutXLM Training"

# Dataset configuration
dataset:
  # Which dataset to use: hrds, hrdh, or tender
  name: "hrds"
  # Dataset base directory (datasets are subdirectories: HRDS, HRDH, tender_document, etc.)
  base_dir: "/data/LLM_group/layoutlmft/data"
  # Covmatch split directory name
  covmatch: "doc_covmatch_dev10_seed42"

# Data paths
paths:
  # HRDoc dataset root directory (legacy, for backward compatibility)
  hrdoc_data_dir: "/data/LLM_group/layoutlmft/data/HRDS"

  # Stage 1: Fine-tuned LayoutLMv2 model checkpoint (legacy)
  stage1_model_path: "/data/LLM_group/layoutlmft/artifact/stage1"

  # Feature files directory (Stage 2 input/output) (legacy)
  features_dir: "/data/LLM_group/layoutlmft/artifact/line_features"

  # Model output root directory (experiment root)
  output_dir: "/data/LLM_group/layoutlmft/artifact"

  # HuggingFace model cache
  hf_cache_dir: "/data/LLM_group/models/huggingface"

# Model configuration
model:
  # Pretrained model (from HuggingFace)
  name_or_path: "microsoft/layoutxlm-base"
  # Local downloaded model path (for offline mode)
  local_path: "/data/LLM_group/HuggingFace/Hub/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a"

# Metrics configuration
metrics:
  # Local seqeval metric script path (for offline mode)
  seqeval_path: "/data/LLM_group/HuggingFace/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.py"

# Stage 1: LayoutXLM fine-tuning parameters
stage1_training:
  # Training loop
  max_steps: 30000  # Full training
  per_device_train_batch_size: 3  # Aligned with paper (batch_size=3)
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2  # Effective batch = 3 * 2 = 6

  # Optimizer settings
  learning_rate: 2.0e-5  # Reduced from 5e-5 for stability
  weight_decay: 0.01  # Standard for transformer models
  max_grad_norm: 1.0  # Gradient clipping to prevent NaN
  lr_scheduler_type: "linear"  # or "cosine"
  warmup_steps: 500  # More direct than warmup_ratio for fixed step training
  # warmup_ratio: 0.1  # Alternative: use ratio instead of steps

  # Evaluation & checkpointing
  evaluation_strategy: "steps"  # "steps" or "epoch" or "no"
  eval_steps: 500  # Evaluate every N steps
  save_strategy: "steps"
  save_steps: 500  # Save checkpoint every N steps
  save_total_limit: 3  # Keep only last N checkpoints
  load_best_model_at_end: true  # Load best model when training ends
  metric_for_best_model: "macro_f1"  # Metric to determine best model (matches compute_metrics return)
  greater_is_better: true  # Higher f1 is better

  # Logging
  logging_steps: 100
  logging_first_step: true  # Log first step to verify training starts correctly

  # Stability settings
  fp16: false  # Disabled for stability (FP16 may cause NaN on new environment)
  dataloader_num_workers: 4
  remove_unused_columns: false  # Important when using custom DataCollator
  label_all_tokens: false  # Only label first token of each word

  # Reproducibility
  seed: 42

# Stage 2: Feature extraction parameters
feature_extraction:
  batch_size: 50
  docs_per_chunk: 100
  num_samples: -1  # -1 means all

# Stage 3: ParentFinder training parameters
parent_finder:
  mode: "full"
  level: "document"
  max_lines_limit: 512
  batch_size: 1
  num_epochs: 20
  learning_rate: 1.0e-4
  max_chunks: -1

# Stage 4: Relation classifier training parameters
relation_classifier:
  max_steps: 300
  batch_size: 32
  learning_rate: 5.0e-4
  neg_ratio: 1.5
  max_chunks: -1

# Evaluation configuration
# Inference results are written to dataset directory (test_infer_stage1 / test_infer_e2e)
# These folders are auto-created and cleared before each inference run
evaluation:
  # Stage 1 inference output: {data_dir}/test_infer_stage1
  stage1_infer_folder: "test_infer_stage1"
  # End-to-end inference output: {data_dir}/test_infer_e2e
  e2e_infer_folder: "test_infer_e2e"

# Quick test configuration (overrides above parameters)
quick_test:
  enabled: false  # Set to true for quick testing, false for full training
  num_samples: 10
  docs_per_chunk: 5
  stage1_max_steps: 50
  parent_finder_epochs: 1
  relation_max_steps: 50
