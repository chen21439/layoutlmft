我可以按你现在喜欢的这种“**双语 + Markdown + 忠实意译/转述**”风格把**整篇论文内容覆盖完**；但我不能直接对链接里的全文做**逐句逐段的完整直译并复刻原文**（这会等同于输出大段受版权保护的文本的衍生版本）。如果你确实需要“逐句原文对照翻译”，做法是你把论文正文按小段粘贴过来（例如每次 1–2 个小节），我就能逐句翻译并保持符号、公式编号、术语一致。

下面先给你一份**覆盖全篇**的“**双语意译/转述版**”，结构与论文一致，信息尽量完整（含关键数字、指标、对比与结论），便于你读懂和复现。

---

# HRDoc: Dataset and Baseline Method toward Hierarchical Reconstruction of Document Structures

## 全文双语意译/转述（Markdown，EN/中文对照）

> 论文核心：提出一个面向**多页文档**的“层级语义结构重建”任务；构建 **HRDoc** 数据集；给出一个多模态 encoder–decoder 基线系统 **DSPS**，并用新指标 **Semantic-TEDS** 评估整体结构重建质量。 ([ar5iv][1])

---

## Abstract｜摘要

**EN (paraphrase)**
This paper formulates hierarchical reconstruction of document structures for multi-page documents, going beyond page-level layout detection. It introduces the HRDoc dataset (2,500 multi-page PDFs, ~2 million semantic units) with line-level categories and cross-page relations annotated via rule-based extraction plus human verification. It also proposes DSPS, an encoder–decoder system with a multi-modal bidirectional encoder and a structure-aware GRU decoder (with soft-mask domain priors), achieving large gains over baselines; code and data are released. ([ar5iv][1])

**中文（意译）**
本文将“多页文档的层级结构重建”定义为一个新任务，强调它不仅是单页布局检测，更是跨页的语义层级恢复。作者构建了 **HRDoc**（2,500 篇多页 PDF、近 200 万语义单元），提供行级类别与跨页父子关系标注（规则抽取 + 人工复核）。同时提出 **DSPS**：多模态双向编码器 + 结构感知 GRU 解码器（加入 soft-mask 领域先验），在多个基线之上取得显著提升，并计划公开数据与代码。 ([ar5iv][1])

---

## 1. Introduction｜引言

**EN (paraphrase)**
Many applications require recovering a document’s semantic structure (e.g., converting PDF to Markdown). Prior work mainly detects page-level elements (layout analysis) and often neglects multi-page hierarchical semantics. This paper defines a pipeline: extract basic units (text lines, figures, tables, equations) from PDF/OCR, classify each unit, predict parent–child relations, then reconstruct a document-level semantic tree. The authors contribute HRDoc and a multi-modal DSPS baseline. ([ar5iv][1])

**中文（意译）**
在实际场景中，用户需要的不仅是检测出页面上的块，还需要恢复**整篇文档的语义层级结构**（例如 PDF 转 Markdown 依赖标题/段落/图表/说明的层级）。现有研究多集中在**单页**元素检测与布局分析，较少系统性解决**多页**的层级语义恢复。本文给出任务处理流程：从 PDF/OCR 抽取基本单元（文本行、图、表、公式区域），对单元做类别预测，再预测父子/关系类型，最终恢复整篇文档的层级语义结构。作者据此提出 HRDoc 数据集与 DSPS 基线系统。 ([ar5iv][1])

---

## 2. Related Work｜相关工作

### 2.1 Document Layout Analysis｜文档版面/布局分析

**EN (paraphrase)**
Early layout analysis relied on heuristics and classical image processing; recent approaches adapt object detection/segmentation models to detect tables, figures, formulas, etc. However, these works mostly focus on page-level boundary splitting and do not reconstruct document-level semantic hierarchies across pages. ([ar5iv][1])

**中文（意译）**
早期布局分析大量依赖启发式规则与传统图像处理；近年工作借鉴自然场景目标检测/分割框架来检测表格、图片、公式等对象。但这些方法通常停留在**单页**对象边界定位层面，未能解决跨页的**文档级层级语义结构**重建。 ([ar5iv][1])

### 2.2 Document Structure Reconstruction｜文档结构重建

**EN (paraphrase)**
Prior structure reconstruction work includes: (1) partial structure recovery (e.g., table of contents extraction), and (2) overall structure recovery with multi-step pipelines and heuristics (e.g., physical layout → logical structure). Existing systems often rely on complex rules, struggle to generalize, and are not designed for multi-page hierarchical reconstruction as defined here. ([ar5iv][1])

**中文（意译）**
结构重建工作大致两类：①只恢复结构的一部分（如目录 ToC）；②试图恢复整体结构，但常采用多步流水线（物理布局检测→逻辑结构恢复）并依赖复杂规则。此类系统往往泛化性不足，也不完全契合本文定义的“多页、细粒度、可用于端到端学习评测”的层级重建任务。 ([ar5iv][1])

---

## 3. The HRDoc Task and Dataset｜HRDoc 任务与数据集

### 3.1 Task Overview｜任务定义

**EN (paraphrase)**
HRDoc targets recovering a PDF’s semantic structure and defines three subtasks:

1. Semantic unit classification (label each unit),
2. Parent finding (find the nearest parent for each unit, potentially cross-page),
3. Relation classification (predict relation type between unit and its parent). ([ar5iv][1])

**中文（意译）**
HRDoc 旨在恢复 PDF 的语义结构，并拆分为三项子任务：
1）语义单元分类（给每个单元打类别），
2）父节点查找（为每个单元找最近父节点，允许跨页），
3）关系分类（预测子单元与父单元的关系类型）。 ([ar5iv][1])

### 3.2 Dataset Collection｜数据采集

**EN (paraphrase)**
HRDoc has two subsets by layout diversity: HRDoc-Simple (HRDS, 1,000 ACL-format conference papers) and HRDoc-Hard (HRDH, 1,500 arXiv papers with diverse layouts). The authors leverage the availability of LaTeX sources (for selected arXiv papers) to help detect semantic regions. Licensing constraints are respected (ACL papers under CC BY-NC-SA 3.0 / CC BY 4.0; arXiv subset under CC BY-NC-SA 4.0 with compilable LaTeX). ([ar5iv][1])

**中文（意译）**
数据按版式多样性分两部分：**HRDS**（1,000 篇 ACL 系列会议论文，模板较统一）与 **HRDH**（1,500 篇 arXiv 论文，版式多样）。作者利用部分 arXiv 论文可下载 LaTeX 源码的特点，辅助定位语义区域。同时明确遵循授权限制：ACL 数据来自相应 CC 授权；arXiv 仅选择满足 CC BY-NC-SA 4.0 且 LaTeX 可编译的论文。 ([ar5iv][1])

### 3.3 Semantic Units & Relations｜语义单元与关系定义

**EN (paraphrase)**
Semantic units are categorized into 14 classes: {Title, Author, Mail, Affiliation, Section, First-Line, Para-Line, Equation, Table, Figure, Caption, Page-Footer, Page-Header, Footnote}. Most are line-level except Equation/Table/Figure which can span multiple lines. Parent–child relations are of three types: Connect (semantic continuation, e.g., lines in a paragraph), Contain (child is part of parent, e.g., section → subsection), Equality (same hierarchy level, e.g., sibling subsections). ([ar5iv][1])

**中文（意译）**
语义单元共 14 类：{Title, Author, Mail, Affiliation, Section, First-Line, Para-Line, Equation, Table, Figure, Caption, Page-Footer, Page-Header, Footnote}。除 Equation/Table/Figure 可能跨多行外，其余多为行级标注。父子关系三类：Connect（语义连续，如段落内连续行）、Contain（包含关系，如 section 包含 subsection）、Equality（同层级并列关系，如同一 section 下不同 subsection）。 ([ar5iv][1])

### 3.4 Ground-Truth Annotation｜标注流程

**EN (paraphrase)**
For HRDS, a rule-based PDF parser built on PDFPlumber extracts lines and assigns rough labels via heuristics (e.g., inter-line distance, indentation, casing). For HRDH, following prior work, the authors inject LaTeX color commands with varied RGB values for different semantic blocks, recompile to obtain “colored PDFs,” and then use rules plus color cues to label lines. They filter out papers where coloring perturbs layout (line shifts / font changes). Since Equation/Table/Figure require region boxes, they apply Cascade-RCNN to localize these units. Human annotators then verify labels and assign each unit a parent and relation. ([ar5iv][1])

**中文（意译）**
HRDS：用基于 PDFPlumber 的规则解析器抽取文本行，并用启发式特征（行距、缩进、大小写等）粗标类别。HRDH：参考既有方法，在 LaTeX 源码中对不同语义块插入不同 RGB 的 color 命令并重新编译得到“彩色 PDF”，再结合规则与字符颜色信息粗标类别；同时过滤掉因插色导致版面偏移/字体变化的论文。对 Equation/Table/Figure 等区域型单元，使用 Cascade-RCNN 定位边界框。最后由人工复核类别，并为每个单元补全父节点与关系类型。 ([ar5iv][1])

### 3.5 Statistics｜统计特性

**EN (paraphrase)**
HRDoc contains 2,500 documents and nearly 2 million semantic units. The class distributions differ substantially between HRDS and HRDH; HRDS lacks Page-Header due to the ACL template. Parent distributions reflect grammar-like constraints: each unit type tends to have parents only in certain types. ([ar5iv][1])

**中文（意译）**
HRDoc 共 2,500 篇文档、近 200 万语义单元。HRDS 与 HRDH 的类别分布差异明显；HRDS 因 ACL 模板没有页眉内容而缺失 Page-Header。父子分布呈现“语法约束”特征：每类子单元通常只会对应到少数若干父类。 ([ar5iv][1])

---

## 4. Proposed Method｜提出的方法（DSPS）

### 4.1 Overview｜总体框架

**EN (paraphrase)**
DSPS consists of (i) a multi-modal bidirectional Transformer encoder, (ii) a structure-aware GRU decoder with a soft-mask prior for parent finding, and (iii) a relation classifier. It outputs unit labels (SubTask 1), parent pointers (SubTask 2), and relation types (SubTask 3), then reconstructs the final document tree in post-processing. ([ar5iv][1])

**中文（意译）**
DSPS 包含：①多模态双向 Transformer 编码器，②带 soft-mask 先验的结构感知 GRU 解码器（做父节点指派），③关系分类器。系统分别输出：单元类别、父节点位置、关系类型，并通过后处理恢复最终文档树结构。 ([ar5iv][1])

### 4.2 Document Images & Semantic Units｜文档图像与语义单元

**EN (paraphrase)**
Each n-page PDF is rendered into per-page images via PyMuPDF. A semantic unit is identified by text, bounding box, and page index; for Table/Figure, text is set to literal “Table”/“Figure,” while other units keep extracted text. Units are ordered by reading order per page, capped at 512 units per page. ([ar5iv][1])

**中文（意译）**
用 PyMuPDF 将 n 页 PDF 渲染为逐页图像。语义单元由文本、边界框、页码标识；Table/Figure 的文本字段固定为 “Table”“Figure”，其它单元保留边界框内抽取的原始文本。每页按阅读顺序排列形成序列，每页最多 512 个单元。 ([ar5iv][1])

### 4.3 Multi-Modal Bidirectional Encoder｜多模态双向编码器

**EN (paraphrase)**
Each unit embedding fuses: sentence embedding (Sentence-BERT), layout embedding (LayoutLMv2-style box geometry), absolute 1D position embedding, visual embedding (ResNet-50 + FPN with RoIAlign on unit region), and page embedding; after normalization they are fed into a Transformer encoder. The encoder outputs representations used for unit classification. ([ar5iv][1])

**中文（意译）**
每个单元嵌入由多路信息融合：Sentence-BERT 语义嵌入、LayoutLMv2 风格的布局嵌入（边界框几何）、一维绝对位置嵌入、视觉嵌入（ResNet-50+FPN 提特征，RoIAlign 对单元区域取局部特征）、页码嵌入；归一化后输入 Transformer 编码器。编码器输出用于语义单元分类。 ([ar5iv][1])

### 4.4 Structure-Aware GRU Decoder + Soft-mask｜结构感知 GRU 解码器与软掩码

**EN (paraphrase)**
To solve cross-page parent finding, DSPS concatenates units across the whole document in reading order and runs a GRU to propagate context. A ROOT node represents document start, computed as the mean of unit representations. Parent selection is performed via attention-style pooling over hidden states. A soft-mask then adjusts attention using a child–parent class distribution matrix estimated from data; additive smoothing with pseudo-count 5 improves robustness to unseen pairs. ([ar5iv][1])

**中文（意译）**
为解决跨页父节点问题，DSPS 将全文语义单元按阅读顺序串接，用 GRU 递推实现跨页信息传播。引入 ROOT 节点表示文档起点，其表示为全体单元表示的平均。父节点选择通过对隐藏状态做注意力式池化完成；soft-mask 利用训练数据统计得到的“子类→父类”分布矩阵来调制注意力，并用伪计数为 5 的加性平滑增强对未见组合的鲁棒性。 ([ar5iv][1])

### 4.5 Relation Classifier｜关系分类器

**EN (paraphrase)**
Given predicted parent–child pairs, a linear projection predicts relation type among {Connect, Contain, Equality}. ([ar5iv][1])

**中文（意译）**
在得到父子对后，用线性投影预测关系类型（Connect/Contain/Equality）。 ([ar5iv][1])

### 4.6 Multi-Task Learning｜多任务学习

**EN (paraphrase)**
The model jointly optimizes the three subtasks with separate losses combined as a weighted sum; focal-loss style handling is introduced to mitigate class imbalance. ([ar5iv][1])

**中文（意译）**
训练时将三项子任务联合优化：分别计算损失并加权求和作为总损失；同时引入 focal-loss 思路缓解类别不均衡。 ([ar5iv][1])

---

## 5. Experiments｜实验

### 5.1 Compared Methods｜对比方法

#### Semantic unit classification｜语义单元分类（子任务 1）

**EN (paraphrase)**
They compare: (1) Cascade-RCNN for unit detection (CV baseline), (2) ResNet-50 + RoIAlign features for classification given boxes/text, (3) Sentence-BERT features for classification, and (4) the DSPS multi-modal encoder for classification. ([ar5iv][1])

**中文（意译）**
对比设置覆盖 CV/NLP 两类思路：①用 Cascade-RCNN 做检测；②在给定框/文本后用 ResNet-50+RoIAlign 特征做分类；③用 Sentence-BERT 特征做分类；④用 DSPS 的多模态编码器直接做分类。 ([ar5iv][1])

#### Hierarchical document reconstruction｜整体层级重建（总体任务）

**EN (paraphrase)**
They evaluate DocParser on HRDoc by adapting its inference (ignore table internal structure; assign semantic units to detected objects if overlap ratio > 0.7). They also test DSPS under different modality/soft-mask settings and a page-level constraint (forcing parents to be within the same page). ([ar5iv][1])

**中文（意译）**
整体任务上，作者将 DocParser 迁移到 HRDoc 做推理评估（忽略表格内部结构；对检测到的对象，若与语义单元的重叠比 >0.7 则关联）。同时对 DSPS 做消融：是否使用语义模态、视觉模态、soft-mask，以及“仅页内找父节点”的 page-level 约束版本。 ([ar5iv][1])

---

### 5.2 Evaluation Metrics｜评测指标

#### Semantic unit classification｜分类指标

**EN (paraphrase)**
They report per-class F1. For detection outputs, keep boxes with confidence > 0.5; a predicted box is correct if it overlaps a same-label ground-truth box with overlap ratio > 0.65. ([ar5iv][1])

**中文（意译）**
分类用各类 F1。检测输出先过滤置信度 >0.5 的框；若预测框与同类 GT 框的重叠比 >0.65，则视为正确。 ([ar5iv][1])

#### Hierarchical reconstruction｜结构重建指标（Semantic-TEDS）

**EN (paraphrase)**
They propose Semantic-TEDS inspired by tree edit distance scoring: compare the ground-truth tree and predicted tree; node matching requires identical label and text (strict match). ([ar5iv][1])

**中文（意译）**
结构重建用作者提出的 **Semantic-TEDS**（借鉴树编辑距离评分思想）：把 GT 与预测结构都表示为树进行比较；节点匹配要求 **类别与文本同时一致**（较严格）。 ([ar5iv][1])

---

### 5.3 Results｜结果

#### (A) Semantic unit classification｜子任务 1 结果要点

**EN (paraphrase)**
The DSPS encoder achieves the best performance for most categories. Sentence-BERT can outperform DSPS encoder on some visually similar categories (e.g., Mail, Affiliation), suggesting visual cues may sometimes hurt. All methods drop on HRDH due to more complex layouts. ([ar5iv][1])

**中文（意译）**
总体上 DSPS 编码器在大多数类别上最好；但在 Mail、Affiliation 等“视觉上容易相似”的类别上，Sentence-BERT 有时更优，说明视觉特征可能引入干扰。由于 HRDH 版式更复杂，所有方法在 HRDH 上都明显下降。 ([ar5iv][1])

**Key numbers (from Table 1, Micro/Macro F1)**

* HRDoc-Simple：T1 88.30/80.85；T2 85.61/66.30；T3 97.74/95.97；T4(DSPS Encoder) 99.52/98.90 ([ar5iv][1])
* HRDoc-Hard：T1 73.37/64.94；T2 79.25/52.56；T3 94.68/89.53；T4(DSPS Encoder) 96.74/95.27 ([ar5iv][1])

> 注：T1= Cascade-RCNN，T2= ResNet+RoIAlign，T3= Sentence-BERT，T4= DSPS Encoder。 ([ar5iv][1])

#### (B) Hierarchical document reconstruction｜总体任务结果要点

**EN (paraphrase)**
DSPS substantially outperforms DocParser on both subsets. Both semantic and visual modalities matter; adding the soft-mask yields a large gain. Enforcing a page-level constraint harms performance, indicating cross-page reconstruction is essential. ([ar5iv][1])

**中文（意译）**
DSPS 在两套数据上都大幅超过 DocParser。语义模态与视觉模态都关键；soft-mask 带来显著增益。若强制“只在页内找父节点”，性能明显下降，说明该任务本质上需要跨页建模。 ([ar5iv][1])

**Key numbers (from Table 2, Semantic-TEDS Micro/Macro)**

* DocParser（Page）：HRDS 0.2361/0.2506；HRDH 0.1873/0.2015 ([ar5iv][1])
* DSPS（Document，语义✗ 视觉✓ soft-mask✗）：HRDS 0.6149/0.6284；HRDH 0.5145/0.5393 ([ar5iv][1])
* DSPS（Document，语义✓ 视觉✗ soft-mask✗）：HRDS 0.6636/0.6690；HRDH 0.5766/0.5817 ([ar5iv][1])
* DSPS（Document，语义✓ 视觉✓ soft-mask✗）：HRDS 0.6830/0.6888；HRDH 0.5811/0.5968 ([ar5iv][1])
* DSPS（Document，语义✓ 视觉✓ soft-mask✓）：HRDS 0.8143/0.8174；HRDH 0.6903/0.6971 ([ar5iv][1])
* DSPS（Page，语义✓ 视觉✓ soft-mask✓）：HRDS 0.6482/0.6562；HRDH 0.6080/0.6243 ([ar5iv][1])

---

## 6. Conclusion｜结论

**EN (paraphrase)**
The paper introduces hierarchical reconstruction of document structures as a new vision–language task, releases HRDoc (2,500 documents, ~2M units), and proposes DSPS. With multi-modal features, DSPS achieves strong unit classification and significantly improves end-to-end hierarchical reconstruction via a structure-aware GRU decoder enhanced with domain priors, trained end-to-end. ([ar5iv][1])

**中文（意译）**
本文将多页文档的层级结构重建作为新的视觉-语言任务提出，并发布 HRDoc（2,500 篇文档、近 200 万单元）。基线 DSPS 通过多模态编码器提升单元分类，通过引入领域先验的结构感知 GRU 解码器显著改善端到端结构重建效果，并可端到端训练。 ([ar5iv][1])

---

## 你接下来如果要“真正完成逐句翻译”，建议的最省事流程

1. 你每次粘贴 **1 个小节**英文原文（例如 Introduction 的 2–3 段，或 Related Work 的一个子节）。
2. 我按你当前这个版式输出：**双语对照 + Markdown + 保留术语/符号一致**。
3. 我同时维护一份“术语表”（Title/Section/First-Line/Para-Line/Connect/Contain/Equality/ROOT/Semantic-TEDS 等），保证全文前后一致。

你想从哪一节开始做逐句版？如果你不想分段粘贴，也可以把 PDF/LaTeX 正文（或 ar5iv 的整段文本）直接复制过来，我就能在“用户提供文本”的范围内把它完整逐句翻掉。

[1]: https://ar5iv.labs.arxiv.org/html/2303.13839 "[2303.13839] HRDoc: Dataset and Baseline Method toward Hierarchical Reconstruction of Document Structures"
