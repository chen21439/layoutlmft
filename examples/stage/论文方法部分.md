### Structure-Aware GRU Decoder
### 结构感知 GRU 解码器

**English:**
When reconstructing the hierarchical structure of one multi-page document, we often meet the cross-page parent-finding problem, which means a semantic unit may find its parent in previous pages. To address this problem, we first concatenate each semantic unit within a document in reading order and use a GRU (Cho et al. 2014) network to capture the information exchange across pages.

**中文：**
在重建多页文档的层级结构时，我们经常遇到跨页寻找父节点的问题，即一个语义单元的父节点可能位于之前的页面中。为了解决这个问题，我们将文档内的每个语义单元按阅读顺序进行拼接，并使用 GRU (Cho et al. 2014) 网络来捕捉跨页面的信息交互。

---

**English:**
When decoding the parent unit, some semantic unit may directly link to the ROOT unit, which represents the beginning of one document. The $x_{0}^{*}$ referring to the multi-modal representation of the Root unit is obtained using the average of all semantic units' representations:
$$x_{0}^{*}=\frac{1}{L}\sum_{i=1}^{L}x_{i}^{*}$$

**中文：**
在解码父单元时，某些语义单元可能直接连接到 ROOT（根）单元，该单元代表文档的起始。代表 Root 单元多模态表示的 $x_{0}^{*}$ 是通过计算所有语义单元表示的平均值得出的：
$$x_{0}^{*}=\frac{1}{L}\sum_{i=1}^{L}x_{i}^{*}$$

---

**English:**
With multi-modal representations $x_{i}^{*}$ as input, the GRU unit produces the hidden state $h_{i}$ of the current semantic unit:
$$h_{i}=GRU(x_{i-1}^{*},h_{i-1})$$

**中文：**
以多模态表示 $x_{i}^{*}$ 作为输入，GRU 单元生成当前语义单元的隐藏状态 $h_{i}$：
$$h_{i}=GRU(x_{i-1}^{*},h_{i-1})$$

---

**English:**
In decoding step $i$, we can calculate the weighted hidden state of $u_{i}$ using the attention pooling function:
$$q_{i}=\sum_{j=0}^{i}\alpha(h_{i},h_{j})h_{j}$$
where
$$\alpha(h_{i},h_{j})=softmax((W_{q}h_{i})^{T}\cdot W_{k}h_{j})=\frac{exp((W_{q}h_{i})^{T}\cdot W_{k}h_{j})}{\sum_{j=0}^{i}exp((W_{q}h_{i})^{T}\cdot W_{k}h_{j})}$$

**中文：**
在解码步骤 $i$ 中，我们可以使用注意力池化函数计算 $u_{i}$ 的加权隐藏状态：
$$q_{i}=\sum_{j=0}^{i}\alpha(h_{i},h_{j})h_{j}$$
其中
$$\alpha(h_{i},h_{j})=softmax((W_{q}h_{i})^{T}\cdot W_{k}h_{j})=\frac{exp((W_{q}h_{i})^{T}\cdot W_{k}h_{j})}{\sum_{j=0}^{i}exp((W_{q}h_{i})^{T}\cdot W_{k}h_{j})}$$

---

**English:**
**The soft-mask operation.** We introduce a soft-mask operation to adjust the attention distribution for better use of domain-specific knowledge. More specifically, we calculate the child-parent distribution matrix $M_{cp}\in\mathbb{R}^{(C+1)\times C}$ by counting different types of child-parent pairs as shown in Figure 4. The $i$-th column of $M_{cp}$ means the probability distribution of class $i$ over $C$ predefined classes plus one ROOT node.

**中文：**
**软掩膜操作 (Soft-Mask Operation)。** 我们引入了一种软掩膜操作来调整注意力分布，以便更好地利用领域特定知识。具体而言，我们通过统计图 4 中所示的不同类型的“子-父”配对，计算出子-父分布矩阵 $M_{cp}\in\mathbb{R}^{(C+1)\times C}$。$M_{cp}$ 的第 $i$ 列表示类别 $i$ 在 $C$ 个预定义类别加上一个 ROOT 节点上的概率分布。

---

**English:**
To make the distribution robust to unseen relation pairs, we apply additive smoothing to each column with the pseudo-count set to 5. The probability of semantic unit $u_{j}$ being the parent of $u_{i}$ can be calculated as:
$$P_{par_{(i,j)}}=softmax(\alpha(q_{i},h_{j})h_{j}P_{dom_{(i,j)}})$$
Here, $P_{dom_{(i,j)}}$ is defined as:
$$P_{dom_{(i,j)}}=P_{cls_{j}}M_{cp}P_{cls_{i}}^{T}$$
where
$$P_{cls_{j}}=\begin{cases} \text{Concat}(P_{cls_{j}}, 0) & j \in \{1, 2, ..., i-1\} \\ [0, 0, ..., 0, 1] & j=0 \end{cases}$$

**中文：**
为了使分布对未见过的关系对具有鲁棒性，我们对每一列应用加法平滑 (additive smoothing)，伪计数 (pseudo-count) 设为 5。语义单元 $u_{j}$ 作为 $u_{i}$ 父节点的概率可以通过以下公式计算：
$$P_{par_{(i,j)}}=softmax(\alpha(q_{i},h_{j})h_{j}P_{dom_{(i,j)}})$$
这里，$P_{dom_{(i,j)}}$ 定义为：
$$P_{dom_{(i,j)}}=P_{cls_{j}}M_{cp}P_{cls_{i}}^{T}$$
其中
$$P_{cls_{j}}=\begin{cases} \text{Concat}(P_{cls_{j}}, 0) & j \in \{1, 2, ..., i-1\} \\ [0, 0, ..., 0, 1] & j=0 \end{cases}$$

---

**English:**
We can calculate the parent node $\hat{P}_{i}\in\{0,1,\cdot\cdot\cdot,i-1\}$ for $u_{i}$ as:
$$\hat{P}_{i}=argmax(P_{par_{(i,j)}})$$

**中文：**
我们可以计算出 $u_{i}$ 的父节点 $\hat{P}_{i}\in\{0,1,\cdot\cdot\cdot,i-1\}$ 为：
$$\hat{P}_{i}=argmax(P_{par_{(i,j)}})$$