

# 4.2. Detect Module (检测模块)

**English:**
The proposed Detect module consists of three primary components: 1) A shared visual backbone network designed to extract multi-scale feature maps from input document images; 2) A top-down graphical page object detection model for detecting graphical page objects, such as tables, figures, and displayed formulas; 3) A bottom-up text region detection model that groups text-lines located outside graphical page objects into text regions, based on the intra-region reading order, and identifies the logical role of each text region.

**Chinese:**
所提出的检测模块（Detect module）由三个主要部分组成：1）一个共享的视觉骨干网络，用于从输入文档图像中提取多尺度特征图；2）一个自顶向下的图形页面对象检测模型，用于检测图形页面对象，如表格、插图和独立公式；3）一个自底向上的文本区域检测模型，该模型根据区域内的阅读顺序，将位于图形页面对象之外的文本行分组为文本区域，并识别每个文本区域的逻辑角色。

**English:**
The overall architecture of the Detect module is illustrated in Fig. 3. In our conference paper [10], we selected a ResNet-50 network as the backbone network to generate multi-scale feature maps and the DINO [69] as the top-down graphical page object detector to localize these graphical objects. However, any suitable visual backbone network and object detection or instance segmentation model can be readily incorporated into our Detect module.

**Chinese:**
检测模块的整体架构如图 3 所示。在我们的会议论文 [10] 中，我们选择 ResNet-50 网络作为骨干网络来生成多尺度特征图，并使用 DINO [69] 作为自顶向下的图形页面对象检测器来定位这些图形对象。然而，任何合适的视觉骨干网络和目标检测或实例分割模型都可以很容易地集成到我们的检测模块中。

**English:**
In this paper, we primarily concentrate on the details of the newly proposed Bottom-up Text Region Detection Model. A text region is a semantic unit of writing that comprises a group of text-lines arranged in natural reading order and associated with a logical label, such as paragraph, list/list-item, title, section heading, header, footer, footnote, and caption.

**Chinese:**
在本文中，我们主要关注新提出的“自底向上文本区域检测模型”的细节。文本区域是一个书写语义单元，由一组按自然阅读顺序排列的文本行组成，并关联一个逻辑标签，如段落、列表/列表项、标题、章节标题、页眉、页脚、脚注和说明文字。

**English:**
Given a document page rendering  composed of  text-lines , the objective of our bottom-up text region detection model is to group these text-lines into distinct text regions according to the intra-region reading order and to recognize the logical role of each text region.

**Chinese:**
给定一个由  个文本行  组成的文档页面渲染 ，我们自底向上文本区域检测模型的目标是根据区域内阅读顺序将这些文本行分组为不同的文本区域，并识别每个文本区域的逻辑角色。

**English:**
In this study, we assume that the bounding boxes and textual contents of text-lines have already been provided by a PDF parser or OCR engine. Based on the detection results of the top-down graphical page object detection model, we initially filter out those text-lines located inside graphical page objects and then utilize the remaining text-lines as input.

**Chinese:**
在本研究中，我们假设文本行的边界框和文本内容已由 PDF 解析器或 OCR 引擎提供。基于自顶向下图形页面对象检测模型的检测结果，我们首先过滤掉那些位于图形页面对象内部的文本行，然后利用剩余的文本行作为输入。

**English:**
As depicted in Fig. 4, our bottom-up text region detection model consists of a multi-modal feature extraction module, a multi-modal feature enhancement module, and two prediction heads, i.e., an intra-region reading order relation prediction head and a logical role classification head. The detailed illustrations of the multi-modal feature enhancement module and the two prediction heads can be found in Fig. 5.

**Chinese:**
如图 4 所示，我们的自底向上文本区域检测模型包括一个多模态特征提取模块、一个多模态特征增强模块和两个预测头，即区域内阅读顺序关系预测头和逻辑角色分类头。多模态特征增强模块和这两个预测头的详细示意图见图 5。

---

### 4.2.1. Multi-modal Feature Extraction Module (多模态特征提取模块)

**English:**
In this module, we extract the visual embedding, text embedding, and 2D Positional Embedding for each text-line.

**Chinese:**
在该模块中，我们为每个文本行提取视觉嵌入、文本嵌入和 2D 位置嵌入。

**English:**
**Visual Embedding.** As shown in Fig. 4, we first resize  and  to the size of  and then concatenate these three feature maps along the channel axis, which are fed into a  convolutional layer to generate a feature map  with 256 channels. For each text-line , we adopt the RoIAlign algorithm [6] to extract  feature maps from  based on its bounding box  where ,  represent the coordinates of its upper left and bottom right corners, respectively. The final visual embedding  of  can be represented as:

**Chinese:**
**视觉嵌入**。如图 4 所示，我们首先将  和  调整为  的大小，然后沿通道轴拼接这三个特征图，将其输入到一个  卷积层中，生成具有 256 个通道的特征图 。对于每个文本行 ，我们采用 RoIAlign 算法 [6] 根据其边界框  从  中提取  的特征图，其中  和  分别代表其左上角和右下角的坐标。 的最终视觉嵌入  可表示为：

**English:**
where FC is a fully-connected layer with 1,024 nodes and LN represents Layer Normalization [70].

**Chinese:**
其中 FC 是一个拥有 1024 个节点的全连接层，LN 代表层归一化（Layer Normalization）[70]。

**English:**
**Text Embedding.** We leverage the pre-trained language model BERT [71] to extract the text embedding of each text-line. Specifically, we first serialize all the text-lines in a document image into a 1D sequence by reading them in a top-left to bottom-right order and tokenize the text-line sequence into a sub-word token sequence, which is then fed into BERT to get the embedding of each token. After that, we average the embeddings of all the tokens in each text-line  to obtain its text embedding  followed by a fully-connected layer with 1,024 nodes to make the dimension the same as that of :

**Chinese:**
**文本嵌入**。我们利用预训练语言模型 BERT [71] 来提取每个文本行的文本嵌入。具体来说，我们首先按照从左上到右下的顺序读取文档图像中的所有文本行，将其序列化为一维序列，并将该文本行序列分词为子词 token 序列，然后将其输入 BERT 以获取每个 token 的嵌入。之后，我们将每个文本行  中所有 token 的嵌入取平均值，以获得其文本嵌入 ，随后通过一个拥有 1024 个节点的全连接层，使其维度与  相同：

**English:**
**2D Positional Embedding.** For each text-line , we encode its bounding box and size information as its 2D Positional Embedding :

**Chinese:**
**2D 位置嵌入**。对于每个文本行 ，我们将其边界框和尺寸信息编码为其 2D 位置嵌入 ：

**English:**
where  and  represent the width and height of  and the input image, respectively. MLP consists of 2 fully-connected layers with 1,024 nodes, each of which is followed by ReLU.

**Chinese:**
其中  和  分别表示  的宽度和高度以及输入图像的宽度和高度。MLP 由 2 个拥有 1024 个节点的全连接层组成，每层之后都接一个 ReLU 激活函数。

**English:**
For each text-line  we concatenate its visual embedding , text embeddings , and 2D Positional Embedding  to obtain its multi-modal representation :

**Chinese:**
对于每个文本行 ，我们将它的视觉嵌入 、文本嵌入  和 2D 位置嵌入  进行拼接，以获得其多模态表示 ：

**English:**
where FC is a fully-connected layer with 1,024 nodes.

**Chinese:**
其中 FC 是一个拥有 1024 个节点的全连接层。

---

### 4.2.2. Multi-modal Feature Enhancement Module (多模态特征增强模块)

**English:**
As shown in Fig. 5, we use a lightweight Transformer encoder to further enhance the multi-modal representations of text-lines by modeling their interactions with a self-attention mechanism. Each text-line is treated as a token of the Transformer encoder and its multi-modal representation is taken as the input embedding:

**Chinese:**
如图 5 所示，我们使用一个轻量级 Transformer 编码器，通过自注意力机制对文本行之间的交互进行建模，从而进一步增强文本行的多模态表示。每个文本行被视为 Transformer 编码器的一个 token，其多模态表示被用作输入嵌入：

**English:**
where  and  are the input and output embeddings of the Transformer encoder,  is the number of the input text-lines. To save computation, here we only use a 1-layer Transformer encoder, where the head number, dimension of hidden state, and the dimension of feedforward network are set as 12, 768, and 2048, respectively.

**Chinese:**
其中  和  分别是 Transformer 编码器的输入和输出嵌入， 是输入文本行的数量。为了节省计算量，这里我们仅使用 1 层 Transformer 编码器，其中头数（head number）、隐藏状态维度（hidden state dimension）和前馈网络维度（feedforward network dimension）分别设置为 12、768 和 2048。

---

### 4.2.3. Intra-region Reading Order Relation Prediction Head (区域内阅读顺序关系预测头)

**English:**
We propose to use a relation prediction head to predict intra-region reading order relationships between text-lines. Given a text-line , if a text-line  is its succeeding text-line in the same text region, we define that there exists an intra-region reading order relationship  pointing from text-line  to text-line . If text-line  is the last (or only) text-line in a text region, its succeeding text-line is considered to be itself.

**Chinese:**
我们建议使用一个关系预测头来预测文本行之间的区域内阅读顺序关系。给定一个文本行 ，如果文本行  是其在同一文本区域中的后续文本行，我们定义存在一个指向从  到  的区域内阅读顺序关系 。如果文本行  是文本区域中的最后一行（或唯一一行），则其后续文本行被认为是它自己。

**English:**
Unlike many previous methods that consider relation prediction as a binary classification task [42, 45], we treat relation prediction as a dependency parsing task and use a softmax cross-entropy loss to replace the standard binary cross-entropy loss during optimization by following [72]. Moreover, we adopt a spatial compatibility feature introduced in [73] to effectively model spatial interactions between text-lines for relation prediction.

**Chinese:**
与许多将关系预测视为二分类任务的先前方法 [42, 45] 不同，我们参考 [72]，将关系预测视为依存句法分析任务，并在优化过程中使用 softmax 交叉熵损失来代替标准的二元交叉熵损失。此外，我们采用了 [73] 中介绍的空间兼容性特征（spatial compatibility feature），以有效地为关系预测建模文本行之间的空间交互。

**English:**
Specifically, we use a multi-class (i.e., n-class) classifier to calculate a score  to estimate how likely  is the succeeding text-line of  as follows:

**Chinese:**
具体来说，我们使用一个多类别（即 n 类）分类器来计算分数 ，以估计  成为  后续文本行的可能性，如下所示：

**English:**
where each of  and  is a single fully-connected layer with 2,048 nodes to map  and  into different feature spaces;  denotes dot product operation; MLP consists of 2 fully-connected layers with 1,024 nodes and 1 node respectively;  is a spatial compatibility feature vector between  and  which is a concatenation of three 6-d vectors:

**Chinese:**
其中  和  均为拥有 2048 个节点的单层全连接层，用于将  和  映射到不同的特征空间； 表示点积运算；MLP 由分别拥有 1024 个节点和 1 个节点的 2 层全连接层组成； 是  和  之间的空间兼容性特征向量，它由三个 6 维向量拼接而成：

**English:**
where  is the union bounding box of  and ;  represents the box delta between any two bounding boxes. Taking  as an example, , where each dimension is given by:

**Chinese:**
其中  是  和  的联合边界框（union bounding box）； 表示任意两个边界框之间的框差（box delta）。以  为例，，其中每个维度的计算方式为：

**English:**
where  and  are the center coordinates of  and , respectively. We select the highest score from scores  and output the corresponding text-line as the succeeding text-line of .

**Chinese:**
其中  和  分别是  和  的中心坐标。我们从分数  中选择最高分，并输出相应的文本行作为  的后续文本行。

**English:**
To achieve higher relation prediction accuracy for the intra-region reading order relationship, which has a chain structure, we employ an additional relation prediction head to further identify the preceding text-line for each text-line. The prediction results from both relation prediction heads are then combined to obtain the final results. Based on the predicted intra-region reading order relationships, we group text-lines into text regions using a Union-Find algorithm. The bounding box of the text region is the union bounding box of all its constituent text-lines.

**Chinese:**
为了在链式结构的区域内阅读顺序关系上获得更高的预测准确率，我们使用了一个额外的关系预测头来进一步识别每个文本行的前序文本行。然后结合两个关系预测头的预测结果以获得最终结果。基于预测的区域内阅读顺序关系，我们使用并查集（Union-Find）算法将文本行分组为文本区域。文本区域的边界框是其所有组成文本行的联合边界框。

---

### 4.2.4. Logical Role Classification Head (逻辑角色分类头)

**English:**
Given the enhanced multi-modal representations of text-lines , we add a multi-class classifier to predict a logical role label for each text-line and determine the logical role of each text region by the plurality voting of all its constituent text-lines.

**Chinese:**
给定增强后的文本行多模态表示 ，我们添加一个多类别分类器来预测每个文本行的逻辑角色标签，并通过其所有组成文本行的多数投票（plurality voting）来确定每个文本区域的逻辑角色。

# 4.3. Order Module (排序模块)

**English:**
The Order module focuses on determining the reading sequence of graphical page objects and text regions identified by the Detect module within document D. Similar to the bottom-up text region detection model employed in the Detect module, we also utilize our proposed multi-modal, transformer-based relation prediction model to predict the inter-region reading order relationships among the recognized page objects.

**Chinese:**
[cite_start]排序模块专注于确定检测模块在文档 D 中识别出的图形页面对象和文本区域的阅读顺序。与检测模块中采用的自底向上文本区域检测模型类似，我们也利用我们提出的多模态、基于 Transformer 的关系预测模型来预测已识别页面对象之间的区域间阅读顺序关系 [cite: 429]。

**English:**
The Order module processes the detected page objects as input and employs an attention-based approach to integrate the features of text-lines belonging to the same text region, thereby achieving a more efficient feature representation of the text region. Furthermore, we define two categories of inter-region reading order relationships: (1) Text region reading order relationships between main body text regions, (2) Graphical region reading order relationships between captions/footnotes and graphical page objects such as tables and figures.

**Chinese:**
[cite_start]排序模块将检测到的页面对象作为输入，并采用基于注意力的方法来整合属于同一文本区域的文本行的特征，从而实现对文本区域更高效的特征表示。此外，我们将区域间阅读顺序关系定义为两类：（1）正文文本区域之间的文本区域阅读顺序关系；（2）说明文字/脚注与图形页面对象（如表格和插图）之间的图形区域阅读顺序关系 [cite: 430, 431]。

**English:**
Consequently, we incorporate an additional inter-region reading order relation classification head to predict relation types. A detailed illustration of the Order module can be found in Fig. 6.

**Chinese:**
[cite_start]因此，我们引入了一个额外的区域间阅读顺序关系分类头来预测关系类型。排序模块的详细示意图见图 6 [cite: 432, 433]。

---

### 4.3.1. Multi-modal Feature Extraction Module (多模态特征提取模块)

**English:**
Following Eqs. (1) and (3) as described in Section 4.2.1, we fuse the visual embedding and the 2D positional embedding to obtain a multi-modal representation $U_{O_{m}}$ for each graphical page object $O_{m}$ in a similar manner.

**Chinese:**
[cite_start]沿用 4.2.1 节中描述的公式 (1) 和 (3)，我们以类似的方式融合视觉嵌入和 2D 位置嵌入，从而获得每个图形页面对象 $O_{m}$ 的多模态表示 $U_{O_{m}}$ [cite: 435]。

**English:**
For each detected text region page object $O_{n}$ consisting of text-lines $[t_{n_{1}},t_{n_{2}},...,t_{n_{k}}]$, we propose an attention fusion model to integrate the features of text-lines $[F_{t_{n_{1}}},F_{t_{n_{2}}},...,F_{t_{n_{k}}}]$ produced by Eq. (5), thereby forming a multi-modal representation $U_{O_{n}}$ for this text region as follows:

**Chinese:**
[cite_start]对于每个由文本行 $[t_{n_{1}},t_{n_{2}},...,t_{n_{k}}]$ 组成的被检测文本区域页面对象 $O_{n}$，我们提出了一种注意力融合模型，用于整合由公式 (5) 生成的文本行特征 $[F_{t_{n_{1}}},F_{t_{n_{2}}},...,F_{t_{n_{k}}}]$，从而形成该文本区域的多模态表示 $U_{O_{n}}$，具体如下 [cite: 436, 437]：

$$
\alpha_{t_{n_{j}}}=FC_{1}(tanh(FC_{2}(F_{t_{n_{j}}}) )), \quad (10)
$$

$$
w_{t_{n_{j}}}=\frac{exp~\alpha_{t_{n_{j}}}}{\sum_{j}exp~\alpha_{t_{n_{j}}}} \quad (11)
$$

$$
U_{O_{n}}=\sum_{j}w_{t_{n_{j}}}F_{t_{n_{j}}} \quad (12)
$$

**English:**
where both $FC_{1}$ and $FC_{2}$ are single fully-connected layers with 1,024 and 1 nodes, respectively. Furthermore, for each page object, we derive a region type embedding for each page object as follows:

**Chinese:**
[cite_start]其中 $FC_{1}$ 和 $FC_{2}$ 分别是拥有 1024 个节点和 1 个节点的单层全连接层。此外，对于每个页面对象，我们按如下方式导出其区域类型嵌入 [cite: 442, 443]：

$$
R_{O_{i}}=LN(ReLU(FC(Embedding(r_{O_{i}})))), \quad (13)
$$

**English:**
where Embedding is an embedding layer with 1,024 hidden dimension and $r_{O_{i}}$ is the logical role of the page object $O_{i}$. Lastly, we concatenate each page object's multi-modal representation $U_{O}$ and region type embedding $R_{O}$ to obtain its final representation $\hat{U}_{O}$ as follows:

**Chinese:**
[cite_start]其中 Embedding 是一个隐藏维度为 1024 的嵌入层，$r_{O_{i}}$ 是页面对象 $O_{i}$ 的逻辑角色。最后，我们将每个页面对象的多模态表示 $U_{O}$ 与区域类型嵌入 $R_{O}$ 进行拼接，以获得其最终表示 $\hat{U}_{O}$，如下所示 [cite: 446, 448]：

$$
\hat{U}_{O_{i}}=FC(Concat(U_{O_{i}},R_{O_{i}})), \quad (14)
$$

**English:**
where FC is a fully-connected layer with 1,024 nodes.

**Chinese:**
[cite_start]其中 FC 是一个拥有 1024 个节点的全连接层 [cite: 450]。

---

### 4.3.2. Multi-modal Feature Enhancement Module (多模态特征增强模块)

**English:**
As illustrated in Fig. 6, we adopt a similar approach to previous multi-modal feature enhancement module in the Group stage. In this case, we utilize a three-layer Transformer encoder to further improve the multi-modal representations of page objects by modeling their interactions using a self-attention mechanism.

**Chinese:**
[cite_start]如图 6 所示，我们采用与此前分组阶段（Detect 阶段）中的多模态特征增强模块类似的方法。在这种情况下，我们利用一个三层 Transformer 编码器，通过自注意力机制对页面对象的交互进行建模，从而进一步改进其多模态表示 [cite: 453, 454]。

**English:**
Each page object is treated as a token of the Transformer encoder, and its multi-modal representation serves as the input embedding:

**Chinese:**
[cite_start]每个页面对象被视为 Transformer 编码器的一个 token，其多模态表示作为输入嵌入 [cite: 455]：

$$
F_{O}=TransformerEncoder(\hat{U}_{O}), \quad (15)
$$

**English:**
where $\hat{U}_{O}=[\hat{U}_{O_{1}},\hat{U}_{O_{2}},...,\hat{U}_{O_{n}}]$ and $F_{O}=[F_{O_{1}},F_{O_{2}},...,F_{O_{n}}]$ represent the input and output embeddings of the Transformer encoder, and n is the number of the input page objects. The hyperparameters of the transformer encoder are consistent with those in the Detect module, except for the layer number.

**Chinese:**
[cite_start]其中 $\hat{U}_{O}=[\hat{U}_{O_{1}},\hat{U}_{O_{2}},...,\hat{U}_{O_{n}}]$ 和 $F_{O}=[F_{O_{1}},F_{O_{2}},...,F_{O_{n}}]$ 分别代表 Transformer 编码器的输入和输出嵌入，n 是输入页面对象的数量。除层数外，Transformer 编码器的超参数与检测模块中的保持一致 [cite: 480, 481]。

---

### 4.3.3. Inter-region Reading Order Relation Prediction Head (区域间阅读顺序关系预测头)

**English:**
Owing to the similarity between the inter-region reading order task of the Order module and the intra-region reading order task of the Detect module, we employ an identical structure for the inter-region reading order relation prediction head in both modules. Further details about this head can be found in Section 4.2.3.

**Chinese:**
[cite_start]由于排序模块的区域间阅读顺序任务与检测模块的区域内阅读顺序任务具有相似性，我们在两个模块中为区域间阅读顺序关系预测头采用了相同的结构。关于该预测头的更多细节可见 4.2.3 节 [cite: 483, 484]。

---

### 4.3.4. Inter-region Reading Order Relation Classification Head (区域间阅读顺序关系分类头)

**English:**
We employ a multi-class classifier to compute the probability distribution across various classes in order to determine the relation type between page object $O_{i}$ and page object $O_{j}$. It works as follows:

**Chinese:**
[cite_start]我们采用一个多类别分类器来计算各个类别的概率分布，以确定页面对象 $O_{i}$ 和页面对象 $O_{j}$ 之间的关系类型。其工作原理如下 [cite: 486]：

$$
p_{ij}=BiLinear(FC_{q}(F_{O_{i}}),FC_{k}(F_{O_{j}})), \quad (16)
$$

$$
c_{ij}=argmax(p_{ij}) \quad (17)
$$

**English:**
where both $FC_{q}$ and $FC_{k}$ represent single fully-connected layers with 2,048 nodes, which are used to map $F_{O_{i}}$ and $F_{O_{j}}$ into distinct feature spaces; BiLinear signifies the bilinear classifier; and argmax refers to identifying the index $c_{ij}$ of the maximum value within the given probability distribution $p_{ij}$ as the predicted relation type.

**Chinese:**
[cite_start]其中 $FC_{q}$ 和 $FC_{k}$ 均表示拥有 2048 个节点的单层全连接层，用于将 $F_{O_{i}}$ 和 $F_{O_{j}}$ 映射到不同的特征空间；BiLinear 表示双线性分类器；argmax 指的是将给定概率分布 $p_{ij}$ 中最大值的索引 $c_{ij}$ 识别为预测的关系类型 [cite: 489, 490]。